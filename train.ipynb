{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from itertools import cycle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ssvae_fixmatch\n",
    "from model import dict2namespace\n",
    "from util import parse_cmd, setup_logger, AverageMeter, make_dir\n",
    "from datasets.get_data import get_cifar10, get_cifar100\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_GETTERS = {\n",
    "    'cifar10': get_cifar10,\n",
    "    'cifar100': get_cifar100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint, dset, n_labeled, augtype, filename='checkpoint.pth.tar'):\n",
    "    file_path = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, file_path)\n",
    "    if is_best:\n",
    "        best_model_filename = f\"{dset}-{n_labeled}_{augtype}_model_best.pth.tar\"\n",
    "        shutil.copyfile(file_path, os.path.join(checkpoint, best_model_filename))\n",
    "\n",
    "def test(data_loader, model, args):\n",
    "    model.eval()\n",
    "    correct_num = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for x, y in data_loader:\n",
    "        x, y = x.to(args.device), y.to(args.device)\n",
    "        with torch.no_grad():\n",
    "            logits = model.encode_y(x)\n",
    "        pred_label = torch.argmax(logits, dim=-1)\n",
    "        v = (pred_label == y).sum()\n",
    "\n",
    "        correct_num += v.item()\n",
    "        total_samples += x.size(0)\n",
    "        \n",
    "    acc = float(correct_num) / total_samples\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img(dset, n_labeled, augtype, device, img_num = 10,\n",
    "                 best_model_dir = \"./checkpoint\",\n",
    "                 labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"],\n",
    "                 save_dir = \"checkpoint\"\n",
    "                ):\n",
    "    best_model_filename = f\"{dset}-{n_labeled}_{augtype}_model_best.pth.tar\"\n",
    "    checkpoint_dir_filename = os.path.join(best_model_dir, best_model_filename)\n",
    "    model = ssvae_fixmatch(args)\n",
    "    model = model.to(device)\n",
    "    checkpoint = torch.load(checkpoint_dir_filename)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    label_num = len(labels)\n",
    "    labels_list = []\n",
    "    for labels_i in range(label_num):\n",
    "        labels_list = labels_list + [labels_i] * img_num\n",
    "    target = torch.tensor(labels_list)\n",
    "    one_hot = torch.nn.functional.one_hot(target)\n",
    "    one_hot = one_hot.to(device)\n",
    "    img_gen = model.generate_sample(one_hot)\n",
    "    \n",
    "    fig, axs = plt.subplots(label_num, img_num, sharex=True, sharey=True, figsize=(15,15))\n",
    "    fig.subplots_adjust(wspace=0.075, hspace=0.075)\n",
    "    for axs_x in range(img_num):\n",
    "        for axs_y in range(label_num):\n",
    "            img_idx = img_num * axs_y + axs_x\n",
    "            img_ij = img_gen.cpu().detach().numpy()\n",
    "            img_ndarray_ij = img_ij[img_idx, :, :, :].transpose((1, 2, 0))\n",
    "            axs[axs_y, axs_x].imshow(img_ndarray_ij)\n",
    "            axs[axs_y, axs_x].axis('off')\n",
    "            if axs_x == 0:\n",
    "                label_i = labels[axs_y]\n",
    "                axs[axs_y, axs_x].text(-1, 0.5, label_i, horizontalalignment='center',\n",
    "                                       verticalalignment='center', transform=axs[axs_y, axs_x].transAxes, fontsize = 25)\n",
    "    fig_filename = f\"{dset}-{n_labeled}_{augtype}_generation.pdf\"\n",
    "    fig_dir_filename = os.path.join(save_dir, fig_filename)\n",
    "    plt.savefig(fig_dir_filename, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, labeled_trainloader, unlabeled_trainloader, model: ssvae_fixmatch, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    sup_losses = AverageMeter()\n",
    "    unsup_losses = AverageMeter()\n",
    "    fixmatch_losses = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    train_loader = zip(cycle(labeled_trainloader), unlabeled_trainloader)\n",
    "    model.train()\n",
    "\n",
    "    n_iter = len(unlabeled_trainloader)\n",
    "    p_bar = tqdm(range(n_iter))\n",
    "\n",
    "    for batch_idx, (data_x, data_u) in enumerate(train_loader):\n",
    "        inputs_x, targets_x = data_x\n",
    "        (inputs_u, inputs_u_w, inputs_u_s), _ = data_u\n",
    "        # move data to gpu\n",
    "        inputs_x = inputs_x.to(args.device)\n",
    "        targets_x = targets_x.to(args.device)\n",
    "        inputs_u = inputs_u.to(args.device)\n",
    "        inputs_u_w = inputs_u_w.to(args.device)\n",
    "        inputs_u_s = inputs_u_s.to(args.device)\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        sup_loss, unsup_loss, fixmatch_loss = model.compute_loss(\n",
    "            inputs_x, targets_x, inputs_u, inputs_u_w, inputs_u_s\n",
    "        )\n",
    "\n",
    "        # compute total loss\n",
    "        loss = sup_loss + unsup_loss + args.mu * fixmatch_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        losses.update(loss.item())\n",
    "        sup_losses.update(sup_loss.item())\n",
    "        unsup_losses.update(unsup_loss.item())\n",
    "        fixmatch_losses.update(fixmatch_loss.item())\n",
    "\n",
    "        p_bar.set_description('Training Epoch: {epoch}/{epochs:4}, Iter: {batch:4}/{iter:4}, Data: {data:.3f}s, Batch: {bt:.3f}s, loss: {loss:.3f}, sup_loss: {sup_loss:.3f}, unsup_loss: {unsup_loss:.3f}, fixmatch_loss: {fixmatch_loss:.3f}'.format(\n",
    "            epoch=epoch + 1,\n",
    "            epochs=args.n_epochs,\n",
    "            batch=batch_idx + 1,\n",
    "            iter=n_iter,\n",
    "            data=data_time.avg,\n",
    "            bt=batch_time.avg,\n",
    "            loss=losses.avg,\n",
    "            sup_loss=sup_losses.avg,\n",
    "            unsup_loss=unsup_losses.avg,\n",
    "            fixmatch_loss=fixmatch_losses.avg\n",
    "        ))\n",
    "        p_bar.update()\n",
    "\n",
    "    p_bar.close()\n",
    "\n",
    "    return losses.avg, sup_losses.avg, unsup_losses.avg, fixmatch_losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 02:29:17,033 - INFO - train -   {'augtype': 'weak', 'batch_size': 64, 'beta1': 0.9, 'beta2': 0.999, 'checkpoint': './checkpoint', 'device': 'cuda:0', 'drop_rate': 0, 'dset': 'cifar10', 'gen': True, 'img_size': 32, 'learning_rate': 0.001, 'log_dir': './experiment', 'model': 'wide_resnet', 'mu': 2, 'n_class': 10, 'n_epochs': 2, 'n_labeled': 4000, 'nc': 3, 'num_workers': 2, 'seed': 0, 'threshold': 0.95, 'use_cuda': True, 'wresnet_k': 2, 'wresnet_n': 28, 'z_dim': 128}\n",
      "2020-11-18 02:29:17,033 - INFO - train -   {'augtype': 'weak', 'batch_size': 64, 'beta1': 0.9, 'beta2': 0.999, 'checkpoint': './checkpoint', 'device': 'cuda:0', 'drop_rate': 0, 'dset': 'cifar10', 'gen': True, 'img_size': 32, 'learning_rate': 0.001, 'log_dir': './experiment', 'model': 'wide_resnet', 'mu': 2, 'n_class': 10, 'n_epochs': 2, 'n_labeled': 4000, 'nc': 3, 'num_workers': 2, 'seed': 0, 'threshold': 0.95, 'use_cuda': True, 'wresnet_k': 2, 'wresnet_n': 28, 'z_dim': 128}\n",
      "2020-11-18 02:29:17,033 - INFO - train -   {'augtype': 'weak', 'batch_size': 64, 'beta1': 0.9, 'beta2': 0.999, 'checkpoint': './checkpoint', 'device': 'cuda:0', 'drop_rate': 0, 'dset': 'cifar10', 'gen': True, 'img_size': 32, 'learning_rate': 0.001, 'log_dir': './experiment', 'model': 'wide_resnet', 'mu': 2, 'n_class': 10, 'n_epochs': 2, 'n_labeled': 4000, 'nc': 3, 'num_workers': 2, 'seed': 0, 'threshold': 0.95, 'use_cuda': True, 'wresnet_k': 2, 'wresnet_n': 28, 'z_dim': 128}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 02:29:18,728 - INFO - train -   ***** Start Training *****\n",
      "2020-11-18 02:29:18,728 - INFO - train -   ***** Start Training *****\n",
      "2020-11-18 02:29:18,728 - INFO - train -   ***** Start Training *****\n",
      "2020-11-18 02:29:18,730 - INFO - train -     Task = cifar10@4000_weak\n",
      "2020-11-18 02:29:18,730 - INFO - train -     Task = cifar10@4000_weak\n",
      "2020-11-18 02:29:18,730 - INFO - train -     Task = cifar10@4000_weak\n",
      "2020-11-18 02:29:18,731 - INFO - train -     Num Epochs = 2\n",
      "2020-11-18 02:29:18,731 - INFO - train -     Num Epochs = 2\n",
      "2020-11-18 02:29:18,731 - INFO - train -     Num Epochs = 2\n",
      "2020-11-18 02:29:18,732 - INFO - train -     Batch Size = 64\n",
      "2020-11-18 02:29:18,732 - INFO - train -     Batch Size = 64\n",
      "2020-11-18 02:29:18,732 - INFO - train -     Batch Size = 64\n",
      "2020-11-18 02:29:18,734 - INFO - train -   Total params: 3.05M\n",
      "2020-11-18 02:29:18,734 - INFO - train -   Total params: 3.05M\n",
      "2020-11-18 02:29:18,734 - INFO - train -   Total params: 3.05M\n",
      "Training Epoch: 1/   2, Iter:  718/ 718, Data: 0.013s, Batch: 0.134s, loss: 2.257, sup_loss: 0.033, unsup_loss: 0.033, fixmatch_loss: 1.095: 100%|██████████| 718/718 [01:36<00:00,  7.47it/s]\n",
      "2020-11-18 02:31:01,039 - INFO - train -   Epoch   1, loss: 2.257, sup_loss: 0.033, unsup_loss: 0.033, fixmatch_loss: 1.095, test_acc: 0.139\n",
      "2020-11-18 02:31:01,039 - INFO - train -   Epoch   1, loss: 2.257, sup_loss: 0.033, unsup_loss: 0.033, fixmatch_loss: 1.095, test_acc: 0.139\n",
      "2020-11-18 02:31:01,039 - INFO - train -   Epoch   1, loss: 2.257, sup_loss: 0.033, unsup_loss: 0.033, fixmatch_loss: 1.095, test_acc: 0.139\n",
      "Training Epoch: 2/   2, Iter:  718/ 718, Data: 0.017s, Batch: 0.140s, loss: 0.950, sup_loss: 0.027, unsup_loss: 0.028, fixmatch_loss: 0.448: 100%|██████████| 718/718 [01:40<00:00,  7.17it/s]\n",
      "2020-11-18 02:33:08,014 - INFO - train -   Epoch   2, loss: 0.950, sup_loss: 0.027, unsup_loss: 0.028, fixmatch_loss: 0.448, test_acc: 0.210\n",
      "2020-11-18 02:33:08,014 - INFO - train -   Epoch   2, loss: 0.950, sup_loss: 0.027, unsup_loss: 0.028, fixmatch_loss: 0.448, test_acc: 0.210\n",
      "2020-11-18 02:33:08,014 - INFO - train -   Epoch   2, loss: 0.950, sup_loss: 0.027, unsup_loss: 0.028, fixmatch_loss: 0.448, test_acc: 0.210\n"
     ]
    }
   ],
   "source": [
    "# parse command\n",
    "# args = parse_cmd()\n",
    "args = {\n",
    "    'batch_size': 64,\n",
    "    'z_dim': 128,\n",
    "    'use_cuda': True,\n",
    "    'device': 'cuda:0',\n",
    "    'wresnet_n': 28,\n",
    "    'wresnet_k': 2,\n",
    "    'drop_rate': 0,\n",
    "    'n_class': 10,\n",
    "    'img_size': 32,\n",
    "    'nc': 3,\n",
    "    'threshold': 0.95,\n",
    "    'mu': 2,\n",
    "    'dset': 'cifar10',\n",
    "    'n_labeled': 4000,\n",
    "    'num_workers': 2,\n",
    "    'learning_rate': 1e-3,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.999,\n",
    "    'n_epochs': 2,\n",
    "    'log_dir': f'./experiment',\n",
    "    'checkpoint': './checkpoint',\n",
    "    'seed': 0,\n",
    "    'model': 'wide_resnet',\n",
    "    'augtype': 'weak',\n",
    "    'gen': True\n",
    "}\n",
    "args = dict2namespace(args)\n",
    "\n",
    "# set up logging\n",
    "logger, writer = setup_logger(args)\n",
    "make_dir(args.checkpoint)\n",
    "\n",
    "# set up random seed\n",
    "if args.seed is not None:\n",
    "    set_seed(args)\n",
    "\n",
    "# set up model config\n",
    "if args.dset == 'cifar10' or args.dset == 'cifar100':\n",
    "    args.img_size = 32\n",
    "    args.nc = 3\n",
    "    args.n_class = int(args.dset[5:])\n",
    "    if args.model == 'wide_resnet':\n",
    "        args.wresnet_n = 28\n",
    "        args.wresnet_k = 2\n",
    "    if args.model == 'resnet':\n",
    "        pass\n",
    "\n",
    "logger.info(dict(args._get_kwargs()))\n",
    "\n",
    "# set up datasets and dataloader\n",
    "labeled_dataset, unlabled_dataset, test_dataset = DATASET_GETTERS[f'{args.dset}'](\n",
    "    './data', args.n_labeled)\n",
    "\n",
    "labeled_train_loader = DataLoader(\n",
    "    labeled_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=RandomSampler(labeled_dataset),\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "unlabeled_trainloader = DataLoader(\n",
    "    unlabled_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=RandomSampler(unlabled_dataset),\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers\n",
    ")\n",
    "\n",
    "# create the model and move to gpu\n",
    "model = ssvae_fixmatch(args)\n",
    "model = model.to(args.device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, betas=(args.beta1, args.beta2))\n",
    "\n",
    "logger.info(\"***** Start Training *****\")\n",
    "logger.info(f\"  Task = {args.dset}@{args.n_labeled}_{args.augtype}\")\n",
    "logger.info(f\"  Num Epochs = {args.n_epochs}\")\n",
    "logger.info(f\"  Batch Size = {args.batch_size}\")\n",
    "logger.info(\"Total params: {:.2f}M\".format(\n",
    "    sum(p.numel() for p in model.parameters()) / 1e6))\n",
    "\n",
    "test_accs = []\n",
    "best_acc = 0\n",
    "model.zero_grad()\n",
    "\n",
    "# best_log_dir = os.path.join(\".\", \"experiment\", f\"{args.dset}@{args.n_labeled}_{args.augtype}\")\n",
    "best_log_filename = f\"log_best_{args.dset}-{args.n_labeled}_{args.augtype}.txt\"\n",
    "best_log_dir_filename = os.path.join(args.log_dir, best_log_filename)\n",
    "with open(best_log_dir_filename, 'w+') as best_log:\n",
    "    for epoch in range(args.n_epochs):\n",
    "    \n",
    "        loss, sup_loss, unsup_loss, fixmatch_loss = train(\n",
    "            args, labeled_train_loader, unlabeled_trainloader, model, optimizer, epoch)\n",
    "    \n",
    "        test_acc = test(test_loader, model, args)\n",
    "    \n",
    "        logger.info(\"Epoch {:3d}, loss: {:.3f}, sup_loss: {:.3f}, unsup_loss: {:.3f}, fixmatch_loss: {:.3f}, test_acc: {:.3f}\".format(\n",
    "            epoch + 1, loss, sup_loss, unsup_loss, fixmatch_loss, test_acc\n",
    "        )\n",
    "                   )\n",
    "    \n",
    "        writer.add_scalar('train/loss', loss, epoch)\n",
    "        writer.add_scalar('train/sup_loss', sup_loss, epoch)\n",
    "        writer.add_scalar('train/unsup_loss', unsup_loss, epoch)\n",
    "        writer.add_scalar('train/fixmatch_loss', fixmatch_loss, epoch)\n",
    "        writer.add_scalar('test/test_acc', test_acc, epoch)\n",
    "    \n",
    "        is_best = test_acc > best_acc\n",
    "        best_acc = max(test_acc, best_acc)\n",
    "        test_accs.append(test_acc)\n",
    "    \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'acc': test_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer': optimizer\n",
    "        }, is_best, args.checkpoint, args.dset, args.n_labeled, args.augtype, filename=f'checkpoint_{args.dset}@{args.n_labeled}_{args.augtype}.pth.tar')\n",
    "        \n",
    "        if is_best:\n",
    "            best_log.write(\"Epoch {:3d}, loss: {:.3f}, sup_loss: {:.3f}, unsup_loss: {:.3f}, fixmatch_loss: {:.3f}, test_acc: {:.3f}\\n\".format(\n",
    "            epoch + 1, loss, sup_loss, unsup_loss, fixmatch_loss, test_acc))\n",
    "            \n",
    "    writer.close()\n",
    "\n",
    "if args.gen == True:\n",
    "    generate_img(args.dset, args.n_labeled, args.augtype, args.device, img_num = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
